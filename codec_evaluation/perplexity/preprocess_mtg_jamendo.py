"""Dataset download: https://github.com/a43992899/MARBLE"""
import os
import pandas as pd
import argparse
from datasets import Dataset, load_from_disk

def create_arrow_from_tsv(meta_dir, split, output_dir):
    """
    Create an Arrow dataset from the TSV file, keeping only the required columns: TAGS, audio_path
        PPL : Can use the dataset files generated by genre because their audios are the same.
        Probe: When generating the datasets required for the other three tasks, simply replace genre with moodtheme/instrument/top50tags . 
               Also, pay attention to the naming of the output folder, which also needs to be modified.
    .arrow file contains two columns:
        - TAGS: tags, Multiple tags are separated by spaces
        - audio_path: Audio file path, in the format of MTG/audio-low/xxxx.low.mp3
    """
    tsv_path = os.path.join(meta_dir, f"data/splits/split-0/autotagging_genre-{split}.tsv")
    
    # Manually read TSV and merge redundant fields
    data = []
    with open(tsv_path, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) < 6:
                print(f"Warning: Line {len(data)+2} does not have enough fields, skipping.")
                continue
            
            # Merge all fields after the fifth column into the TAGS column, using spaces to connect
            tags = ' '.join(parts[5:])  
            record = {
                'PATH': parts[3],   
                'TAGS': tags, 
            }
            data.append(record)
    
    # Convert to DataFrame
    df = pd.DataFrame(data)
    
    # Handle audio path
    df['audio_path'] = "MTG/audio-low/" + df['PATH'].str.replace('.mp3', '.low.mp3')
    
    # Keep only the columns you need
    df = df[['TAGS', 'audio_path']]
    
    # Create a dataset
    dataset = Dataset.from_pandas(df)
    
    # Save in Arrow format
    output_path = os.path.join(output_dir, f"MTGGenre_{split}_dataset")
    dataset.save_to_disk(output_path)
    dataset.to_json(os.path.join(output_dir, f"MTGGenre_{split}_dataset.jsonl"))
    
    print(f"Successfully saved {split} dataset to: {output_path}, {len(dataset)} samples.")
    return dataset

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create MTG Arrow datasets from TSV files.")
    parser.add_argument("--meta_dir", type=str, required=True, help="Path to MTG meta directory")
    parser.add_argument("--output_dir", type=str, required=True, help="Path to save processed dataset")
    parser.add_argument("--splits", type=str, nargs="+", default=["train", "validation", "test"],
                        help="Which splits to process, default: train validation test")

    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)

    for split in args.splits:
        dataset= create_arrow_from_tsv(args.meta_dir, split, args.output_dir)
        
        # Test loading the dataset
        dataset_loaded = load_from_disk(os.path.join(args.output_dir, f"MTGGenre_{split}_dataset"))
        print(f"{split} dataset structure:\n{dataset_loaded}")
        print(f"{split} first three data contents:")
        for example in dataset_loaded.select(range(3)):
            print(example)

        



