{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test hubert few-shot ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "# change working directory to project root\n",
    "os.chdir(\"/home/yrb/code/MusicAudioPretrain/\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import librosa, librosa.display\n",
    "import scipy, matplotlib.pyplot as plt\n",
    "\n",
    "from benchmark.GTZAN.GTZAN_dataset import FeatureDataset as GTZAN_FeatureDataset\n",
    "from benchmark.GS.GS_dataset import FeatureDataset as GS_FeatureDataset\n",
    "\n",
    "random.seed(1234)\n",
    "feature_dir = \"data/GTZAN/hubert_features/HF_model_HuBERT_base_MPD_train_1Kh_valid_300h_iter1_250k_vanilla_model_ncluster_500_feature_layer_all_reduce_mean\"\n",
    "metadata_dir = \"data/GTZAN\"\n",
    "layer = 'all'\n",
    "train_dataset = GTZAN_FeatureDataset(feature_dir, metadata_dir, 'train', layer, return_audio_path=True)\n",
    "valid_dataset = GTZAN_FeatureDataset(feature_dir, metadata_dir, 'valid', layer, return_audio_path=True)\n",
    "test_dataset = GTZAN_FeatureDataset(feature_dir, metadata_dir, 'test', layer, return_audio_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        x = torch.tensor(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    x /= x.norm(dim=-1, keepdim=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure dataset\n",
    "dataset = train_dataset\n",
    "class_data = dict()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    feature, label, audio_path = dataset[i]\n",
    "    if feature.shape[0] != 768:  # cat all layers to one vector\n",
    "        feature = feature.reshape(-1)\n",
    "    if label not in class_data:\n",
    "        class_data[label] = []\n",
    "    class_data[label].append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_centroids(class_data, num_shots):\n",
    "    class_centroids = dict()\n",
    "    for label, features in class_data.items():\n",
    "        features = torch.tensor(np.array(features))\n",
    "        # sample num_shots features\n",
    "        if features.shape[0] > num_shots:\n",
    "            features = features[random.sample(range(features.shape[0]), num_shots)]\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "        class_centroids[label] = features.mean(dim=0, keepdim=True)\n",
    "    return class_centroids\n",
    "\n",
    "\n",
    "all_acc = []\n",
    "repeat_times = 100\n",
    "num_shots = 10\n",
    "# repeat the experiment for many times, since different centroid initialization will lead to different results\n",
    "for _ in tqdm(range(repeat_times)):\n",
    "    # compute class centroids\n",
    "    class_centroids = get_class_centroids(class_data, num_shots)\n",
    "    class_centroids = torch.cat([class_centroids[i] for i in range(10)])\n",
    "    class_centroids = normalize(class_centroids)\n",
    "    results, labels, paths = [], [], []\n",
    "    for feature, label, audio_path in test_dataset:\n",
    "        if feature.shape[0] != 768:  # cat all layers to one vector\n",
    "            feature = feature.reshape(-1)\n",
    "        feature = normalize(feature)\n",
    "        probs = (feature @ class_centroids.T).softmax(dim=-1)\n",
    "        top_prob, top_label = probs.topk(1, dim=-1)\n",
    "        top_label = top_label.item()\n",
    "        results.append(top_label)\n",
    "        labels.append(label)\n",
    "        paths.append(audio_path)\n",
    "    all_acc.append(accuracy_score(labels, results))\n",
    "all_acc = np.array(all_acc)\n",
    "print(f\"Accuracy: {all_acc.mean():.4f} +- {all_acc.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bad case analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred in enumerate(results):\n",
    "    if pred != labels[i]:\n",
    "        print(f\"{paths[i]}: pred = {test_dataset.id2class[pred]}, label = {test_dataset.id2class[labels[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('map')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d71646711130ff2f83d358e7f55653aa736e519194d9cf69d7bb43de7121816"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
